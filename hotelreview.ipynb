{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install gensim\n",
    "#!pip install nltk\n",
    "#!pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-12T08:07:07.941998Z",
     "iopub.status.busy": "2023-09-12T08:07:07.941463Z",
     "iopub.status.idle": "2023-09-12T08:07:07.956064Z",
     "shell.execute_reply": "2023-09-12T08:07:07.954801Z",
     "shell.execute_reply.started": "2023-09-12T08:07:07.941955Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertModel, BertForSequenceClassification, AdamW\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import string\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_cleaning(message):\n",
    "    Test_punc_removed = [char for char in message if char not in string.punctuation]\n",
    "    Test_punc_removed_join = ''.join(Test_punc_removed)\n",
    "    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n",
    "    return Test_punc_removed_join_clean\n",
    "\n",
    "url = 'https://drive.google.com/uc?id={}'.format('1wHdWpDysG4hipEMnqJjGoQ1Oqf9mlzZF')\n",
    "df = pd.read_csv(url)#,nrows=20)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000,analyzer = message_cleaning)  # You can adjust the number of features as needed\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['Review'])\n",
    "\n",
    "y = df[['Rating']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a linear regression model\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"MSE:\",mse)\n",
    "print(\"R2:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = \"nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,  \"\n",
    "new_review_tfidf = tfidf_vectorizer.transform([new_review])\n",
    "predicted_rating = model1.predict(new_review_tfidf)\n",
    "print(predicted_rating)\n",
    "\n",
    "import pickle\n",
    "filename = 'finalized_model1.pkl'\n",
    "pickle.dump(model1, open(filename, 'wb'))\n",
    "\n",
    "y_pred = model1.predict(X_test)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "url = 'https://drive.google.com/uc?id={}'.format('1wHdWpDysG4hipEMnqJjGoQ1Oqf9mlzZF')\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "max_words = 5000  # You can adjust this value as needed\n",
    "max_len = 100      # You can adjust this value as needed\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer2 = Tokenizer(num_words=max_words)\n",
    "tokenizer2.fit_on_texts(df['Review'])\n",
    "X_seq = tokenizer2.texts_to_sequences(df['Review'])\n",
    "X_padded = pad_sequences(X_seq, maxlen=max_len)\n",
    "\n",
    "y = df[['Rating']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a model with an embedding layer and a Dense layer\n",
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "model2.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "model2.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new review text\n",
    "new_review = \"nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,  \"\n",
    "\n",
    "# Tokenize the new review\n",
    "new_review_seq = tokenizer2.texts_to_sequences([new_review])\n",
    "new_review_padded = pad_sequences(new_review_seq, maxlen=max_len)\n",
    "\n",
    "# Make a prediction using the trained model\n",
    "predicted_rating = model2.predict(new_review_padded)\n",
    "\n",
    "# The predicted_rating is a NumPy array, so you can extract the predicted value\n",
    "predicted_rating = predicted_rating[0][0]\n",
    "\n",
    "# Print the predicted rating\n",
    "print(f\"Predicted Rating: {predicted_rating}\")\n",
    "\n",
    "model2.save('finalized_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T08:07:52.773197Z",
     "iopub.status.busy": "2023-09-12T08:07:52.772806Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 8.9199\n"
     ]
    }
   ],
   "source": [
    "url = 'https://drive.google.com/uc?id={}'.format('1wHdWpDysG4hipEMnqJjGoQ1Oqf9mlzZF')\n",
    "df = pd.read_csv(url,nrows=2000)\n",
    "\n",
    "X = df['Review'].tolist()\n",
    "y = df['Rating'].tolist()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)  # Regression with one output neuron\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], torch.tensor(y_val, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Set up the optimizer and loss function for regression\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits.squeeze(1)  # Squeeze to remove extra dimension\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = outputs.logits.squeeze(1)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    mean_val_loss = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {mean_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Perform any necessary post-processing or analysis on test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data if available and save the model\n",
    "test_encodings = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_dataset = TensorDataset(test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "test_predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits.squeeze(1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"bert_regression_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)  # Regression with one output neuron\n",
    "model.load_state_dict(torch.load(\"bert_regression_model.pth\"))  # Replace \"bert_regression_model.pth\" with the path to your trained model\n",
    "\n",
    "model.eval()\n",
    "input_text = \"nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,  \"\n",
    "input_ids = tokenizer(input_text, truncation=True, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(input_ids).logits.item()\n",
    "\n",
    "print(f\"Predicted Rating: {prediction:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T01:33:39.089458Z",
     "iopub.status.busy": "2023-09-12T01:33:39.089057Z",
     "iopub.status.idle": "2023-09-12T01:34:02.781545Z",
     "shell.execute_reply": "2023-09-12T01:34:02.780017Z",
     "shell.execute_reply.started": "2023-09-12T01:33:39.089428Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T01:34:02.786152Z",
     "iopub.status.busy": "2023-09-12T01:34:02.785695Z",
     "iopub.status.idle": "2023-09-12T01:34:02.796960Z",
     "shell.execute_reply": "2023-09-12T01:34:02.795726Z",
     "shell.execute_reply.started": "2023-09-12T01:34:02.786115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T01:34:02.798809Z",
     "iopub.status.busy": "2023-09-12T01:34:02.798430Z",
     "iopub.status.idle": "2023-09-12T01:34:08.120397Z",
     "shell.execute_reply": "2023-09-12T01:34:08.118327Z",
     "shell.execute_reply.started": "2023-09-12T01:34:02.798778Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/uc?id={}'.format('1wHdWpDysG4hipEMnqJjGoQ1Oqf9mlzZF')\n",
    "df = pd.read_csv(url)\n",
    "reviews = df['Review'].values.tolist()\n",
    "\n",
    "# Tokenize and preprocess the reviews\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    return tokens\n",
    "\n",
    "tokenized_reviews = [preprocess(review) for review in reviews]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def preprocessSentiment(words):\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    for word in words:\n",
    "        sentiment = analyzer.polarity_scores(word)\n",
    "        positive_score += sentiment['pos']\n",
    "        negative_score += sentiment['neg']\n",
    "\n",
    "    if positive_score > negative_score:\n",
    "        sentiment = \"Good\"\n",
    "    elif positive_score < negative_score:\n",
    "        sentiment = \"Bad\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "# Create a single dictionary for all reviews\n",
    "dictionary = corpora.Dictionary(tokenized_reviews)\n",
    "\n",
    "# Build an LDA model for each review\n",
    "num_topics = 1  # You can adjust this number based on your requirements\n",
    "\n",
    "# Print the topics and their top words for each review\n",
    "for i, tokens in enumerate(tokenized_reviews):\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "    print(f\"Review {i + 1} Topics:\")\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic[1])\n",
    "        text = topic[1]\n",
    "        words = re.findall(r'\"(.*?)\"', text)\n",
    "        print(words)\n",
    "        sentiment=preprocessSentiment(words)\n",
    "        print(\"sentiment:\", sentiment)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
